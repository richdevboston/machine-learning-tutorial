{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm-Up - MultiLayer Perceptron Classifier\n",
    "In this excercise we will go through scikit-learn high level modeling, as well as TensorFlow computational model. We will go through TensorFlow advanced modeling in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "As a step before running the training and machine learning model, we need to load the data. We will use `sklearn.datasets` to get a sample dataset. [Check this link for details](http://scikit-learn.org/stable/datasets/).\n",
    "The data should have two parts:\n",
    "- `X` - array-like data with `shape = (n_samples, n_features)`. That means the input array has to have `n_samples` rows and `n_features` columns.\n",
    "- `y` - array-like true labels list with `shape = (n_samples, 1)`, which means it has to have the same number of elements as the number of rows in the `X`. *Note: the shape of `y` could be `(n_samples, n_outputs)`*\n",
    "\n",
    "We will use `sklearn.datasets` for `tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab --no-import-all inline\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split # To get the test data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## 1. Load the data\n",
    "X, y = make_circles(n_samples=1000, noise=0.05)\n",
    "\n",
    "## 1.1 Preprocess and split the data\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "## 1.2 Prepare some more data\n",
    "## The {xx, yy} are used to create decision boundaries for demo purposes later\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data to use with TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Change warning: default value of `enable_centered_bias` will change after 2016-10-09. It will be disabled by default.Instructions for keeping existing behaviour:\n",
      "Explicitly set `enable_centered_bias` to 'True' if you want to keep existing behaviour.\n",
      "WARNING:tensorflow:Using default config.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6a4a32016fba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# mlp.fit(input_fn=lambda: input_fn(training_set), steps=5000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# mlp.fit(input_fn=lambda: input_fn(training_set), steps=5000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'X1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'X2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    433\u001b[0m     result = self._estimator.fit(x=x, y=y, input_fn=input_fn, steps=steps,\n\u001b[1;32m    434\u001b[0m                                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmonitors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                                  max_steps=max_steps)\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmonitors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    326\u001b[0m     input_fn, feed_fn = _get_input_fn(x, y, input_fn, feed_fn=None,\n\u001b[1;32m    327\u001b[0m                                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                                       epochs=None)\n\u001b[0m\u001b[1;32m    329\u001b[0m     loss = self._train_model(input_fn=input_fn,\n\u001b[1;32m    330\u001b[0m                              \u001b[0mfeed_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36m_get_input_fn\u001b[0;34m(x, y, input_fn, feed_fn, batch_size, shuffle, epochs)\u001b[0m\n\u001b[1;32m     98\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                                              \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                                              epochs=epochs)\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_builder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feed_dict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.pyc\u001b[0m in \u001b[0;36msetup_train_data_feeder\u001b[0;34m(x, y, n_classes, batch_size, shuffle, epochs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mStreamingDataFeeder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m   return data_feeder_cls(\n\u001b[0;32m--> 117\u001b[0;31m       x, y, n_classes, batch_size, shuffle=shuffle, epochs=epochs)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, n_classes, batch_size, shuffle, random_state, epochs)\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0moutput_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDType\u001b[0m \u001b[0mof\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \"\"\"\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;31m# self.n_classes is None means we're passing in raw target indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     y_dtype = (\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "COLUMNS = ['X'+str(idx+1) for idx in range(np.shape(X)[1])] + ['y']\n",
    "FEATURES = COLUMNS[:-1]\n",
    "LABEL = COLUMNS[-1]\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in FEATURES]\n",
    "# training_set = {'X'+str(idx+1): np.array(X_train[:, idx]) for idx in range(len(FEATURES))}\n",
    "# training_set[LABEL] = np.array(y_train)\n",
    "\n",
    "mlp = tf.contrib.learn.DNNClassifier(feature_columns=feature_cols, hidden_units=[10, 10])\n",
    "\n",
    "def input_fn(data_set):\n",
    "    feature_cols = {k: tf.constant(data_set[k]) for k in FEATURES}\n",
    "    labels = tf.constant(data_set[LABEL])\n",
    "    return feature_cols, labels\n",
    "                                 \n",
    "\n",
    "# mlp.fit(input_fn=lambda: input_fn(training_set), steps=5000)\n",
    "# mlp.fit(input_fn=lambda: input_fn(training_set), steps=5000)\n",
    "mlp.fit(x = {'X1': X_train[:, 0], 'X2': X_train[:, 1]}, y = y_train, steps=5000)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Learn Implementation Flow\n",
    "`scikit-learn` Python package provides a collection of high-level calls to various machine learning algorithms. The beauty of this library is in its simplicity due to high level of abstraction.\n",
    "The library is not suited well for low level modeling, and one should use `SciPy` and `NumPy` for low-level design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Choose a model](http://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "Once you choose a model, you will have to instantiate it using the right arguments. We will discuss the parameters of the machine learning models in the later notebooks. For the current example we have to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(6,), \n",
    "    activation='relu',\n",
    "    max_iter=2000,\n",
    "    tol=1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the chosen model using `.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Predict new values using `.predict()`.\n",
    "Or you can get the probabilities of different labels using `.predict_proba()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = mlp.predict(X_train)\n",
    "print (\"Train accuracy is {0:.2%}\".format(1. - np.mean(np.abs(y_hat - y_train))))\n",
    "y_hat = mlp.predict(X_test)\n",
    "print (\"Test accuracy is {0:.2%}\".format(1. - np.mean(np.abs(y_hat - y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Get the accuracy of the model using `.score()`\n",
    "If you just need the accuracy, you don't need to compute it yourself, and just use the `.score()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_score = mlp.score(X_train, y_train)\n",
    "test_score = mlp.score(X_test, y_test)\n",
    "print (\"Train accuracy is {0:.2%}\".format(train_score))\n",
    "print (\"Test accuracy is {0:.2%}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the MLP using \"Circles\" synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "## Specify the colormap:\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "if hasattr(mlp, \"decision_function\"):\n",
    "    Z = mlp.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "else:\n",
    "    Z = mlp.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "## Plot the test values\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "\n",
    "## Figure parmeters\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "\n",
    "## Title and score report\n",
    "ax.set_title('MLP circles')\n",
    "ax.text(xx.max() - .3, yy.min() + .3, ('{0:.2%}'.format(test_score)),\n",
    "                size=15, horizontalalignment='right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Neuron Visualization\n",
    "We can also visualize the outputs of individual neurons, as an example, let us see what the neurons of the hidden layer really represent. In order to do that, we will use `{xx, yy}` as an input. Note that different activation functions could be used! Also, the outputs of the hidden layers are supposed to be combined in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Step 1: Compute the outputs of the hidden layer\n",
    "hidden_layer_out = np.dot(np.c_[xx.ravel(), yy.ravel()], mlp.coefs_[-2])\n",
    "coef_1 = np.diag(mlp.coefs_[-1][:,0])\n",
    "y_hidden = np.dot(hidden_layer_out, coef_1)\n",
    "\n",
    "idx = 1\n",
    "for y_neuron in y_hidden.T:\n",
    "    y_neuron = y_neuron.reshape(xx.shape)\n",
    "    ax = plt.subplot(2,3,idx)\n",
    "    ax.contourf(xx, yy, y_neuron, cmap=cm, alpha=.8)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(\"Hidden layer %d\"%idx)\n",
    "    idx += 1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
